{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a450b5-3013-4d8a-afe6-e2355e4189df",
   "metadata": {},
   "source": [
    "# LangSmith <> ActionWeaver\n",
    "\n",
    "Integrating LangSmith tracing within a multi-reasoning bot orchestrated by ActionWeaver is seamless due to LangSmith's simplified tracing and debugging capabilities within LLM applications. \n",
    "\n",
    "ActionWeaver, being a framework structured around function callings, naturally lends itself to effortless integration with LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c751e7b2-0065-44d0-9f12-961b4eed56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from uuid import UUID, uuid4\n",
    "\n",
    "from pydantic import BaseModel, Field, PrivateAttr, validate_call\n",
    "\n",
    "from actionweaver import action\n",
    "from actionweaver.utils import DEFAULT_ACTION_SCOPE\n",
    "from actionweaver.llms import wrap, ExceptionHandler, ExceptionAction, ChatLoopInfo, Continue, Return\n",
    "\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "from openai import AzureOpenAI, OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf1013-6298-430d-b326-a674126a637b",
   "metadata": {},
   "source": [
    "Let's initialize an OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "871b70ed-8bd1-4fa3-925a-dcb0f16024a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI llm \n",
    "\n",
    "# Azure OpenAI\n",
    "MODEL=\"gpt-4-32k\"\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2023-10-01-preview\"\n",
    ")\n",
    "\n",
    "# OpenAI\n",
    "# MODEL = \"gpt-4\"\n",
    "# client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1a8c8-f816-43b3-80bd-7fa4bb551751",
   "metadata": {},
   "source": [
    "## Wrap the OpenAI client with ActionWeaver and LangSmith for traceability.\n",
    "\n",
    "ActionWeaver simplifies the development of LLM applications by providing straightforward tools for structured data parsing, function dispatching, and orchestration.\n",
    "\n",
    "To employ ActionWeaver with tracing capabilities.\n",
    "\n",
    "- Wrap the OpenAI client's `client.chat.completions.create` method to enable OpenAI API call tracing.\n",
    "- Wrap the OpenAI client with an ActionWeaver wrapper.\n",
    "- Finally, Wrap the ActionWeaver-wrapped client's `client.create` method to ensure tracing on top level `client.create` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8bb44e-33b2-4d55-bd2c-c1e7424aaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LangSmith Environment\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "project_name = \"actionweaver\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name \n",
    "\n",
    "assert os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "\n",
    "# Apply LangSmith tracing to the original LLM  client's chat completion method.\n",
    "# This allows for detailed tracking of API calls to OpenAI.\n",
    "client.chat.completions.create = traceable(name=\"llm_call\", run_type=\"llm\")(client.chat.completions.create)\n",
    "\n",
    "# Enhance the LLM client with ActionWeaver.\n",
    "llm = wrap(client)\n",
    "\n",
    "# Track ActionWeaver wrapped create method with LangSmith tracing to monitor ActionWeaver calls.\n",
    "llm.create = traceable(name=\"actionweaver_call\", run_type=\"llm\")(llm.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a60f5-249b-49f0-8502-e3ec97eecc5c",
   "metadata": {},
   "source": [
    "Let's create an ActionWeaver action by leveraging the Search Tool from the LangChain community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0762ae06-0d42-455e-8c0a-bf2ba4751272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.google_search.tool import GoogleSearchRun\n",
    "from langchain_community.utilities.google_search import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search_api = GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper())\n",
    "\n",
    "@action(name=\"GoogleSearch\", decorators=[traceable(run_type=\"tool\")])\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform a Google search using the provided query. \n",
    "    \"\"\"\n",
    "    return google_search_api(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40788e15-b87d-4a45-9373-c2361b83fd3d",
   "metadata": {},
   "source": [
    "## Create Pydantic models for multi steps reasoning\n",
    "\n",
    "ActionWeave  natively supports Pydantic for data validation when calling functions. Within this setup, we will define various models that are utilized by the reasoning bot.\n",
    "\n",
    "Each Task model is equipped with an execute method. This method leverages the web search tool, contextualized by the task at hand. The function orchestration is defined as follows:\n",
    "\n",
    "```python\n",
    "orch = {\n",
    "      DEFAULT_ACTION_SCOPE: web_search,\n",
    "      web_search.name: None\n",
    "  }\n",
    "```\n",
    "\n",
    "This orch argument outlines the loop where the LLM calls the web search tool first,  invoke LLM API again, and then delivers the final outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "395ef277-a877-486a-89f8-5bf76d8784f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task models used in multi steps reasoning processing \n",
    "class Task(BaseModel):\n",
    "    \"\"\"Represents a task.\"\"\"\n",
    "    _uid: UUID = PrivateAttr(default_factory=uuid4)\n",
    "    _created_at: datetime = PrivateAttr(default_factory=datetime.now)\n",
    "    task_id: int = Field(..., description=\"Identifier for the task\", examples = [\"1\"])\n",
    "    description: str = Field(..., description=\"A comprehensive and standalone description of the task.\", examples=[\"Create a Python function that takes a list of integers and returns the sum. The function should handle empty lists by returning zero.\"])\n",
    "    dependencies: List[int] = Field([], description=\"Task IDs that this task depends on\", examples=[\"1,2,3\"])\n",
    "\n",
    "    @traceable(run_type=\"tool\")\n",
    "    def execute_task(self, context, llm, eh) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": self.description}\n",
    "        ]\n",
    "        response = llm.create(\n",
    "          model=MODEL,\n",
    "          messages=messages,\n",
    "          stream=False, \n",
    "          orch = {\n",
    "              DEFAULT_ACTION_SCOPE: web_search,\n",
    "              web_search.name: None\n",
    "          },\n",
    "          exception_handler = eh,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "class TaskPlan(BaseModel):\n",
    "    \"\"\"Represents a task plan, outlining the overall problem to be solved and its subtasks.\"\"\"\n",
    "    description: str = Field(..., description=\"Descrption of the overall problem needs to be solved\")\n",
    "    tasks: List[Task] = Field(..., description=\"Subtasks required to solve the problem\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d0949-3b99-42f0-9d17-2201546de149",
   "metadata": {},
   "source": [
    "Here we create an agent `TaskPlanner` that leverage LLM to perform two actions `plan_tasks_and_solve` and `summarize_info` through function callings.\n",
    "\n",
    "To ensure the integrity of the inputs passed to these functions, we're including Pydantic `validate_call` for input validation. Additionally, to manage exceptions and facilitate retries, a simple ExceptionHandler is implemented.\n",
    "\n",
    "\n",
    "The orchestration of function calls is managed by the `orch` parameter, designed as follows:\n",
    "```python\n",
    "orch = {\n",
    "              DEFAULT_ACTION_SCOPE: self.plan_tasks_and_solve,\n",
    "              self.plan_tasks_and_solve.name: self.summarize_info,\n",
    "          }\n",
    "```\n",
    "\n",
    "This configuration instructs the LLM to first execute the plan_tasks_and_solve action, followed by the summarize_info action to condense the results and present them directly to the user. For further details on function orchestration, refer to [ActionWeaver](https://github.com/TengHu/ActionWeaver?tab=readme-ov-file#orchestration-of-actions-experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f015a1cc-3ff9-4a34-acb9-a0ba718c9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPlanner:\n",
    "    def __init__(self, llm, eh):\n",
    "        self.llm = llm\n",
    "        self.debug_info = {}\n",
    "        self.eh = eh\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a task planner, approach the question by breaking it into smaller tasks and addressing each step systematically\"},\n",
    "        ]\n",
    "\n",
    "    def __call__(self, query:str) -> str:\n",
    "        self.messages.append({\"role\": \"user\", \"content\": query})\n",
    "        response = llm.create(\n",
    "          model=MODEL,\n",
    "          messages=self.messages,\n",
    "          stream=False, \n",
    "          exception_handler = self.eh,\n",
    "          orch = {\n",
    "              DEFAULT_ACTION_SCOPE: self.plan_tasks_and_solve,\n",
    "              self.plan_tasks_and_solve.name: self.summarize_info,\n",
    "          }\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    @action(name=\"Summarize\", stop=True, decorators=[traceable(run_type=\"tool\")])\n",
    "    @validate_call\n",
    "    def summarize_info(self, content: str) -> str:\n",
    "        \"\"\"Condense the information to provide a concise response to the question.\"\"\"\n",
    "        return content\n",
    "\n",
    "    @action(name=\"CreateAndExecutePlan\", decorators=[traceable(run_type=\"tool\")])\n",
    "    @validate_call\n",
    "    def plan_tasks_and_solve(self, task_plan: TaskPlan) -> str:\n",
    "        \"\"\"Create and execute a plan for complex problem\"\"\"\n",
    "        self.debug_info[\"task_plan\"] =  task_plan\n",
    "        \n",
    "        id2results = {}\n",
    "        id2tasks = {}\n",
    "        \n",
    "        from graphlib import TopologicalSorter \n",
    "        graph = defaultdict(set)\n",
    "        for task in task_plan.tasks:\n",
    "            graph[task.task_id].update(task.dependencies)\n",
    "            id2tasks[task.task_id] = task\n",
    "    \n",
    "        # topo sort\n",
    "        ts = TopologicalSorter(graph) \n",
    "        tasks_in_order = [*ts.static_order()]\n",
    "        \n",
    "        # execute tasks\n",
    "        for task_id in tasks_in_order:\n",
    "            task = id2tasks[task_id]\n",
    "            context = '\\n'.join([f\"{task.description}:{id2results[dep_id]}\\n\" for dep_id in task.dependencies])\n",
    "            res = task.execute_task(context, self.llm, self.eh)\n",
    "            id2results[task.task_id] = res\n",
    "\n",
    "        self.debug_info[\"id2results\"] =  id2results\n",
    "        return [f\"{id2results[task_id]}\\n\" for task_id in tasks_in_order]\n",
    "\n",
    "class ExceptionRetryHandler(ExceptionHandler):\n",
    "    def __init__(self, retry=2):\n",
    "        self.retry = retry\n",
    "\n",
    "    @traceable(run_type=\"tool\")\n",
    "    def handle_exception(self, e: Exception, info: ChatLoopInfo) -> ExceptionAction:\n",
    "        if self.retry:\n",
    "            self.retry -= 1\n",
    "            return Continue(functions=info.context['functions'])\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6ed0a9-9fd3-4a9f-9338-541f495f07ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "eh = ExceptionRetryHandler(2)\n",
    "task_planner = TaskPlanner(llm, eh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf59620-f6ba-4b85-b077-f1aa5c199ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = task_planner(\"\"\"Discover the establishment year of the university attended by the individual credited with inventing the computer.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d82d709d-5a59-48e0-b1f7-926030c5b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Babbage, often credited with inventing the computer, studied at Cambridge University. Established in 1209, it's one of the world's oldest universities.\n"
     ]
    }
   ],
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14070dcc-e3b7-4e7c-b447-a46dda259504",
   "metadata": {},
   "source": [
    "Let's examine how the final outcome is generated by tracing through LangSmith!\n",
    "\n",
    "<img src=\"./figures/langsmith.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f434904-6269-4136-a02b-56384dc3bd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

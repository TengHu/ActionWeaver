{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02433815-7bb9-4e61-be67-db4854f0c403",
   "metadata": {},
   "source": [
    "# QuickStart\n",
    "\n",
    "_ActionWeaver is an AI application framework that puts function-calling as a first-class feature_.\n",
    "\n",
    "---\n",
    "\n",
    "This demonstrates how to seamlessly integrate any Python function into your LLM application using ActionWeaver and either the Azure or OpenAI API.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e365394b-99c6-49bc-9115-37f25ea83072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16250eaa-15b7-4848-81cf-ac62cd8cefa9",
   "metadata": {},
   "source": [
    "**Use ActionWeaver and OpenAI API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d682d986-a3ac-4f4d-a967-a6eaccef0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from actionweaver.utils.tokens import TokenUsageTracker\n",
    "from actionweaver.llms import patch\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = patch(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22710e02-f81f-4050-8c2c-01cd64e48f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from actionweaver import action\n",
    "\n",
    "@action(name=\"GetCurrentTime\")\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"\n",
    "    Use this for getting the current time in the specified time zone.\n",
    "    \n",
    "    :return: A string representing the current time in the specified time zone.\n",
    "    \"\"\"\n",
    "    print (\"Getting current time...\")\n",
    "    import datetime\n",
    "    current_time = datetime.datetime.now()\n",
    "    \n",
    "    return f\"The current time is {current_time}\"\n",
    "\n",
    "\n",
    "@action(name=\"GetWeather\", stop=False)\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    print (\"Getting current weather\")\n",
    "    \n",
    "    import json\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c84261-b03b-4333-8cea-8315241c69a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current weather\n",
      "Getting current weather\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8olQcSJWBqrdiBevxOTSVt0n0qS19', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The current weather in San Francisco is 72°F. Unfortunately, I couldn't retrieve the current weather for Beijing.\", role='assistant', function_call=None, tool_calls=None))], created=1707109406, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=24, prompt_tokens=153, total_tokens=177))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"what's the weather in San Francisco and Beijing ?\"}\n",
    "  ]\n",
    "\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=messages,\n",
    "    actions = [get_current_weather],\n",
    "    stream=False, \n",
    "    token_usage_tracker = TokenUsageTracker(500),\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1741f-3e0a-4673-9e2d-683d8e51ff47",
   "metadata": {},
   "source": [
    "**Use ActionWeaver and Azure OpenAI Service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12f7870-aa76-4755-ab89-96737ca9a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "azure_client = patch(AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2023-10-01-preview\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f342800e-fd6e-4195-9da9-44a4cc713f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current weather\n",
      "Getting current weather\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8olQhd7zmsGYXe0U5p7Pv7lbWF1re', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The current temperature in San Francisco is 72°F. However, I couldn't retrieve the current temperature in Beijing at the moment.\", role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1707109411, model='gpt-35-turbo-16k', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=27, prompt_tokens=155, total_tokens=182), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"what's the weather in San Francisco and Beijing ?\"}\n",
    "  ]\n",
    "\n",
    "response = azure_client.chat.completions.create(\n",
    "  model=\"gpt-35-turbo-0613-16k\",\n",
    "  messages=messages,\n",
    "  stream=False,\n",
    "  actions = [get_current_weather],\n",
    "  token_usage_tracker = TokenUsageTracker(500),\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70eb220-807d-41f3-afa8-5d1c6305c6cb",
   "metadata": {},
   "source": [
    "**Easily integrate tools from libraries such as [Langchain](https://github.com/langchain-ai/langchain/tree/master/libs/community)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6569a0-c2e1-4109-b88a-8025ed76dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.google_search.tool import GoogleSearchRun\n",
    "from langchain_community.utilities.google_search import GoogleSearchAPIWrapper\n",
    "search_tool = GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568e6e30-362b-4ab6-a088-99370269c205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8olQkSd1ng4l4x1IdEkrkgmiYHGnN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ActionWeaver is an AI application framework that is designed based on the concept of LLM (Language Learning Model) function calling. It is built with the aim of scaling up and orchestrating OpenAI functions. With ActionWeaver, you can easily turn any function into an OpenAI language model function using a simple decorator.\\n\\nThe ActionWeaver project is available on GitHub at: [https://github.com/TengHu/ActionWeaver](https://github.com/TengHu/ActionWeaver)', role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1707109414, model='gpt-35-turbo-16k', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=102, prompt_tokens=436, total_tokens=538), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"search what is ActionWeaver on Github\"}\n",
    "  ]\n",
    "\n",
    "from actionweaver.actions.factories.langchain import action_from_tool\n",
    "\n",
    "response = azure_client.chat.completions.create(\n",
    "  model=\"gpt-35-turbo-0613-16k\",\n",
    "  messages=messages,\n",
    "  stream=False,\n",
    "  actions = [action_from_tool(search_tool, description=\"Invoke this tool to search any information\")],\n",
    "  token_usage_tracker = TokenUsageTracker(500),\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d6414-c04c-4fbe-bfeb-8cb9effc2b39",
   "metadata": {},
   "source": [
    "**Force execute an action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6aa568-cdc5-45bc-8ee5-4c111bc347fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current time...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8olQpnI7bvn6K9HVOWHJzYwLa2Ze4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current time is 00:03:39.', role='assistant', function_call=None, tool_calls=None))], created=1707109419, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=98, total_tokens=110))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_time.invoke(openai_client, messages=[{\"role\": \"user\", \"content\": \"what time\"}], model=\"gpt-3.5-turbo\", stream=False, force=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6635698-7886-414d-9cb9-84f6ad28ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current time...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8olQsrtqkktjMNjXRj1KBItN9uwaY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current time is 00:03:41.', role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1707109422, model='gpt-35-turbo-16k', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=98, total_tokens=110), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_time.invoke(azure_client, messages=[{\"role\": \"user\", \"content\": \"what time\"}], model=\"gpt-35-turbo-0613-16k\", stream=False, force=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9543b-1947-4545-9d8e-7aed7833c565",
   "metadata": {},
   "source": [
    "**Stop the Action in the loop**\n",
    "\n",
    "Every action comes with a stop argument, which is set to False by default, if True this means that the LLM will immediately return the function's output if chosen, but this also restricts the LLM from making multiple function calls. For instance, if asked about the weather in NYC and San Francisco, the model would invoke two separate functions sequentially for each city. However, with `stop=True`, this process is interrupted once the first function returns weather information for either NYC or San Francisco, depending on which city it queries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4178e1d-ef6c-4954-b342-729f73b4736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_current_weather.stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad3616a-18ab-4809-893c-171a0c625245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current weather\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather.invoke(openai_client, messages=[{\"role\": \"user\", \"content\": \"what weather is San Francisco\"}], model=\"gpt-3.5-turbo\", stream=False, force=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02433815-7bb9-4e61-be67-db4854f0c403",
   "metadata": {},
   "source": [
    "# Actions of Stateful Agent\n",
    "Developers also could create a class and enhance its functionality using ActionWeaver's action decorators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c94281-1d38-4e34-bbd0-92ff70227482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from actionweaver.llms import patch\n",
    "from actionweaver import action\n",
    "from typing import List\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d94a0-19ba-4874-8db3-0b1f28230da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentV0:\n",
    "    def __init__(self):\n",
    "        self.llm = patch(OpenAI())\n",
    "        self.messages = []\n",
    "        self.times = []\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        self.messages += [{\"role\": \"user\", \"content\":text}]\n",
    "        return self.llm.chat.completions.create(model=\"gpt-3.5-turbo\", messages=self.messages, actions = [self.get_current_time])\n",
    "        \n",
    "    @action(name=\"GetCurrentTime\")\n",
    "    def get_current_time(self) -> str:\n",
    "        \"\"\"\n",
    "        Use this for getting the current time in the specified time zone.\n",
    "        \n",
    "        :return: A string representing the current time in the specified time zone.\n",
    "        \"\"\"\n",
    "        import datetime\n",
    "        current_time = datetime.datetime.now()\n",
    "\n",
    "        self.times += [str(current_time)]\n",
    "        \n",
    "        return f\"The current time is {current_time}\"\n",
    "\n",
    "agent = AgentV0()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9138451-e0b5-44bd-b768-2c9f25bbcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can invoke actions just like regular instance methods\n",
    "agent.get_current_time() # Output: 'The current time is 20:34.'\n",
    "\n",
    "\n",
    "agent(\"what time is it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381c18a-302f-4976-bd39-5c16fe68e2dd",
   "metadata": {},
   "source": [
    "**Grouping and Extending Actions Through Inheritance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e1792-3210-4be8-85c9-f6c669dfe127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainTools:\n",
    "    @action(name=\"GoogleSearch\")\n",
    "    def google_search(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Perform a Google search using the provided query. \n",
    "        \n",
    "        This action requires `langchain` and `google-api-python-client` installed, and GOOGLE_API_KEY, GOOGLE_CSE_ID environment variables.\n",
    "        See https://python.langchain.com/docs/integrations/tools/google_search.\n",
    "\n",
    "        :param query: The search query to be used for the Google search.\n",
    "        :return: The search results as a string.\n",
    "        \"\"\"\n",
    "        from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "        search = GoogleSearchAPIWrapper()\n",
    "        return search.run(query)\n",
    "    \n",
    "class AgentV1(AgentV0, LangChainTools):\n",
    "    def __call__(self, text):\n",
    "        self.messages += [{\"role\": \"user\", \"content\":text}]\n",
    "        return self.llm.chat.completions.create(model=\"gpt-3.5-turbo\", messages=self.messages, actions = [self.google_search])\n",
    "\n",
    "agent = AgentV1()\n",
    "agent(\"what happened today\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389be3b9-cd9f-4b52-aef1-0745f2164f87",
   "metadata": {},
   "source": [
    "We could use parameter `orch` when calling the chat completion API. This feature will allow us for more precise control over the specific set of tools available to the LLM during each interaction.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "client.chat.completions.create(\n",
    "    messages = ...\n",
    "    actions=[a1, a2, a3], # First, LLM respond with either a1, a2 or a3, or text without action\n",
    "    # Define the orchestration logic for actions:\n",
    "    orch={\n",
    "        a1.name: [a2, a3],  # If a1 is invoked, the next response will be either a2, a3 or a text response.\n",
    "        a2.name: a3,      # If a2 is invoked, the next action will be a3\n",
    "        a3.name: [a4]     # If a3 is invoked, the next response will be a4 or a text response.\n",
    "        a4.name: None     # If a4 is invoked, the next response will guarantee to be a text message\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "For details please take a look at [here](https://github.com/TengHu/ActionWeaver?tab=readme-ov-file#orchestration-of-actions-experimental )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fffbbb7-c8e3-4a98-9564-01ed054c5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileAgent(AgentV0):\n",
    "    @action(name=\"FileHandler\")\n",
    "    def handle_file(self, instruction: str) -> str:\n",
    "        \"\"\"\n",
    "        Handles ALL user instructions related to file operations.\n",
    "    \n",
    "        Args:\n",
    "            instruction (str): The user's instruction about file handling.\n",
    "    \n",
    "        Returns:\n",
    "            str: The response to the user's question.\n",
    "        \"\"\"\n",
    "        print (f\"Handling {instruction}\")\n",
    "        return instruction\n",
    "        \n",
    "\n",
    "    @action(name=\"ListFiles\")\n",
    "    def list_all_files_in_repo(self, repo_path: str ='.') -> List:\n",
    "        \"\"\"\n",
    "        Lists all the files in the given repository.\n",
    "    \n",
    "        :param repo_path: Path to the repository. Defaults to the current directory.\n",
    "        :return: List of file paths.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"list_all_files_in_repo: {repo_path}\")\n",
    "        \n",
    "        file_list = []\n",
    "        for root, _, files in os.walk(repo_path):\n",
    "            for file in files:\n",
    "                file_list.append(os.path.join(root, file))\n",
    "            break\n",
    "        return file_list\n",
    "\n",
    "    @action(name=\"ReadFile\")\n",
    "    def read_from_file(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Reads the content of a file and returns it as a string.\n",
    "    \n",
    "        :param file_path: The path to the file that needs to be read.\n",
    "        :return: A string containing the content of the file.\n",
    "        \"\"\"\n",
    "        print(f\"read_from_file: {file_path}\")\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        return f\"The file content: \\n{content}\"\n",
    "\n",
    "    def __call__(self, text):\n",
    "        self.messages += [{\"role\": \"user\", \"content\":text}]\n",
    "        return self.llm.chat.completions.create(model=\"gpt-3.5-turbo\", messages=self.messages, actions = [self.handle_file], orch = {self.handle_file.name: [self.list_all_files_in_repo, self.read_from_file]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ed4906a-b57e-4f78-b9a1-bea1c2a195ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FileAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e8eca2b-a052-4f38-9f57-3b42cfc362d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling list all files\n",
      "list_all_files_in_repo: .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8atToDzgZh5C9gh4xNWYsJtsiaNPS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are the files in the current repository:\\n\\n- langsmith.ipynb\\n- azure_tutorial-Copy1.ipynb\\n- parallel_tools.log\\n- untitled.md\\n- parallel_tools.ipynb\\n- test.log\\n- stateful_agent.ipynb\\n- huggingface.ipynb\\n- anyscale.ipynb\\n- ReAct.ipynb\\n- structured_extraction.log\\n- quickstart.ipynb\\n- structured_extraction.ipynb\\n- azure_tutorial.ipynb\\n- litellm.ipynb\\n- cookbook.ipynb\\n- agent.log\\n- orchestration.ipynb', role='assistant', function_call=None, tool_calls=None))], created=1703803764, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=130, prompt_tokens=256, total_tokens=386))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"Take file action [list all files in current repository]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d2274-c011-4cb8-9710-da1b34bbbb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
